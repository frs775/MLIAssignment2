{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inports\n",
    "\n",
    "# plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "sns.set()\n",
    "# Define figure sizes\n",
    "plt.rcParams.update({'figure.figsize': (8, 5), 'figure.dpi': 120})\n",
    "\n",
    "# Data management libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from dateutil.parser import parse \n",
    "\n",
    "# Machine Learning libraries\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_predict\n",
    "import scipy as sp\n",
    "\n",
    "# Others\n",
    "import math\n",
    "from mltools import forecast_tools as FT\n",
    "import scipy.stats as st\n",
    "\n",
    "# Non linear models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neural_network import  MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from mltools import model_tools as MT\n",
    "from mltools import regression_tools as RT\n",
    "# from neuralsens import partial_derivatives as ns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "df = pd.read_csv('UnemploymentSpain.dat', sep = '\\t')\n",
    "dates = df['DATE']\n",
    "df.drop(columns=['DATE'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a time series\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for col in df.columns.values.tolist():\n",
    "    ax.plot(col, data=df, label=col, alpha=0.8)\n",
    "ax.set(title='Time series data', ylabel='Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Se puede observar que no es estacionaria en media ni en varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load time series values\n",
    "\n",
    "df_ts = df[['TOTAL']]\n",
    "df_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF and PACF of the time series -> identify significant lags and order\n",
    "\n",
    "plt.figure(figsize=[15,15])\n",
    "FT.ts_display(df_ts)\n",
    "\n",
    "# No estacionaria en media (Plot y ACF)\n",
    "# Se observa cierta estacionalidad en PACF, no podemos identificar estacionalidad a simple vista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ El decrecimiento lento en ACF podria indicar que se trata de una serie integrada\n",
    "+ De nuevo observamos que no es estacionaria en media ni varianza\n",
    "+ Se puede identificar estacionalidad aunque no podemos determinar de forma clara su periodo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Stabilize the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box-cox transformation\n",
    "lmbda = FT.boxcox_lambda_plot(df_ts, window_width=12)\n",
    "\n",
    "# No estacionaria en varianza -> Transformacion Box-cox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Del grafico se observa que la serie no es estacionaria en varianza, por ello realizaremos una transformacion Box-Cox con el valor lambda indicado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Box Cox\n",
    "# Indica si queremos transformacion logaritmica\n",
    "BOX_COX = True\n",
    "if BOX_COX:\n",
    "    # SELECIONAR DE GRAFICO ANTERIOR\n",
    "    lmbda = 0.1608\n",
    "    z = st.boxcox(df_ts.values[:,0], lmbda = lmbda) #Convert to positive\n",
    "    #z,lmbda = st.boxcox(df_ts.values[:,0] - min(df_ts.values) + 1) #Convert to positive and automatic selection of lmbda\n",
    "    z = pd.DataFrame(z, columns=df_ts.columns.values.tolist())\n",
    "else:\n",
    "    z = df_ts\n",
    "\n",
    "plt.figure(figsize=[15,15])\n",
    "FT.ts_display(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Se aprecia levemente que la serie es estacionaria en varinza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Box Cox of transformed series\n",
    "FT.boxcox_lambda_plot(z, window_width=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Tanto el grafico como el valor lambda nos indica que la serie transformada es estacionaria en varianza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative test - Augmented Dickey Fuller Test\n",
    "# SI P-VALOR MAYOR QUE 0.05 NECESITAS DIFERENCIAR\n",
    "result = adfuller(z.values)\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "\tprint('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "# No estacionaria en media -> Diferenciacion regular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ p-valor = 0.46 > alpha, por lo tanto no rechazamos la Hipotesis nula, H_0: {Serie no estacionaria} por ello diferenciaremos la serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference of the time series\n",
    "# Regular\n",
    "d = 1\n",
    "# Estacional\n",
    "D = 1\n",
    "S = 12 # Seasonality of 12 days\n",
    "\n",
    "Bz = z\n",
    "for diff in range(d):\n",
    "    Bz = Bz.diff().dropna() # drop first NA value\n",
    "for seas_diff in range(D):\n",
    "    Bz = Bz.diff(S).dropna() # drop first NA values\n",
    "plt.figure(figsize=[15,15])\n",
    "FT.ts_display(Bz,lags=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Tras realizar una diferenciacion regular se puede apreciar que el periodo de la estacionalidad es 12\n",
    "+ Tras diferenciar estacional y regularmente se siguen observando algunos elementos no modelados correctamente, cerca del valor 100 se podria corresponder a la crisis del año 2008 y cerca del valor 250 al efecto del covid-19, esto se podria solucionar empleando variables impulso (sarimax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative test - Augmented Dickey Fuller Test\n",
    "\n",
    "# SI P-VALOR MAYOR QUE 0.05 NECESITAS DIFERENCIAR\n",
    "result = adfuller(Bz.values)\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "\tprint('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "# Serie Estacionaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Una vez diferenciada la serie, p-valor < alpha y por lo tanto podemos concluir que la serie transformada y diferenciada es estacionaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Fit SARIMA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Regular: AR(1)\n",
    "+ Estacional: MA(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ En base a los graficos ACF y PACF de la serie transformada y diferenciada identificamos el modelo.\n",
    "Para la parte regular nos fijamos el los elementos asociados al perido mas reciente, en este caso, sin tener en cuenta alcunas correlaciones no tan significativas, podemos identificar un AR(1). Para la perte estacionaria, nos fijaremos en las correlaciones asociadas a los periodos, en este caso es claro que se trata de un MA(1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model with estimated order\n",
    "sarima_fit = SARIMAX(z, \n",
    "                    # D: NUM DIFERENCIACIONES HECHAS\n",
    "                    order=(1,1,0), # Regular components (p, d, q)\n",
    "                    seasonal_order=(0, 1, 1, 12), # Seasonal components (p, d, q, s)\n",
    "                    trend= 'n', # Type of trend: ['c','t','n','ct'] --> [constant, linear, no trend, constant and linear]\n",
    "                    enforce_invertibility=False, \n",
    "                    enforce_stationarity=False).fit()\n",
    "\n",
    "print(sarima_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ AIC =   -252.318, buscamos que este valor sea lo más bajo posible\n",
    "+ Por otro lado, se observa que todos los coeficientes del modelo son siginificativos\n",
    "+ Por último, este se trata del modelo SARIMA más simple posible que se adecue al comportamiento de la serie temporal obtenida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residuals analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residual error\n",
    "plt.figure(figsize=[15,15])\n",
    "FT.check_residuals(pd.DataFrame(sarima_fit.resid.loc[50:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Ljung-Box test indica que los residuos son independientes (p-valor > alpha -> No rechazamos H_0:{Residuos independientes})\n",
    "+ Como ya se comentó, existe comportamiento no modelado en la serie temporal, esto se observa en el plot de los residuos, donde cerca del valor 230 observamos que estos son más significativos (tambien cerca del valor 100 pero mas sutil), así como en los ACF y PACF.\n",
    "+ Residuos aproximadamente normal, aparecen outliers en la cola derecha, esto se puede deber a comportamiento no modelado u outliers\n",
    "+ De ACF y PACF se observa que el residuo no es ruido blanco, existen correlaciones significativas -> No se ha modelado todo el comportamiento de la serie temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain forecasts for in-sample and out-of-sample\n",
    "\n",
    "# Inicio y horizonte\n",
    "start = 200\n",
    "horizon = 20\n",
    "end = df_ts.shape[0] + horizon\n",
    "\n",
    "pred = sarima_fit.get_prediction(start=start, end= end, dynamic=False)\n",
    "yhat = pred.predicted_mean\n",
    "yhat_conf_int = pred.conf_int(alpha=0.05)\n",
    "\n",
    "#Undo Box-cox transform if necessary\n",
    "if BOX_COX:\n",
    "    yhat = sp.special.inv_boxcox(yhat, lmbda)\n",
    "    yhat_conf_int = sp.special.inv_boxcox(yhat_conf_int, lmbda)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.fill_between(yhat_conf_int.index,\n",
    "                yhat_conf_int.iloc[:, 0],\n",
    "                yhat_conf_int.iloc[:, 1], color='k', alpha=.2)\n",
    "plt.plot(df_ts.loc[start:])\n",
    "plt.plot(yhat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ En dicho grafico se observa el valor real como el predicho, se observa además la dificultad del modelo para modelar el efecto del covid-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction of out_of_sample and confidence intervals\n",
    "# If using dynamic = True, the forecast are used as real data\n",
    "horizon = 20\n",
    "end = df_ts.shape[0] + horizon\n",
    "\n",
    "# COMANDO PREDICCION\n",
    "pred = sarima_fit.get_forecast(steps=horizon, dynamic=False)\n",
    "yhat = pred.predicted_mean\n",
    "yhat_conf_int = pred.conf_int(alpha=0.05)\n",
    "\n",
    "#Undo Box-cox transform if necessary\n",
    "if BOX_COX:\n",
    "    yhat = sp.special.inv_boxcox(yhat, lmbda)\n",
    "    yhat_conf_int = sp.special.inv_boxcox(yhat_conf_int, lmbda)\n",
    "\n",
    "\n",
    "plt.figure(figsize=[15,15])\n",
    "plt.fill_between(yhat_conf_int.index,\n",
    "                yhat_conf_int.iloc[:, 0],\n",
    "                yhat_conf_int.iloc[:, 1], color='k', alpha=.2)\n",
    "plt.plot(df_ts.loc[1000:])\n",
    "plt.plot(yhat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# November 2022 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediccion para steps = 1 (Escala 1 e6)\n",
    "predPreBC = sarima_fit.get_forecast(steps=1, dynamic=False).predicted_mean\n",
    "if BOX_COX:\n",
    "    pred = sp.special.inv_boxcox(predPreBC, lmbda)\n",
    "\n",
    "print('Prediction:', round(pred.values[0]), 'parados en Noviembre de 2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Emplearemos variables impulso para modelar el comportamiento del Covid-19. Tambien se podrian emplear para modelar el efecto de la crisis del 2008, no obstante el efecto de esta última es modelado de forma más precisa y no supone errores tan significativos\n",
    "\n",
    "+ Partimos de la serie anterior ya transformada y diferenciada, por tanto estacionaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables impulso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear variables impulso\n",
    "# Observando df_ts: Covid-19: 229-241\n",
    "# Crecimiento en diferentes etapas, diferentes variables impulso\n",
    "\n",
    "numExog = 3\n",
    "exp1 = np.zeros(len(df_ts))\n",
    "exp2 = np.zeros(len(df_ts))\n",
    "exp3 = np.zeros(len(df_ts))\n",
    "exp1[228] = 1\n",
    "exp2[229] = 1\n",
    "exp3[230] = 1\n",
    "exp = np.matrix([exp1, exp2, exp3]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Debido a que el efecto de la pandemia se fue desarrollando en distintas etapas, cada una de ellas con un impacto diferente, emplearemos tres variables impulso para corregir el modelado de este comportamiento. Solo lo haremos para el inicio de esta, el proceso de estabilización de la serie es modelado de manera mas precisa, no obstante, tambien se podrian emplear estas variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit SARIMAX model\n",
    "Podemos identificar el modelo SARIMAX del mismo modo que se hizo con el SARIMA\n",
    "+ Regular: AR(1)\n",
    "+ Estacional: MA(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model with estimated order\n",
    "sarimax_fit = SARIMAX(endog = z, \n",
    "                    exog = exp,\n",
    "                    # D: NUM DIFERENCIACIONES HECHAS\n",
    "                    order=(1,1,0), # Regular components (p, d, q)\n",
    "                    seasonal_order=(0, 1, 1, 12), # Seasonal components (p, d, q, s)\n",
    "                    trend= 'n', # Type of trend: ['c','t','n','ct'] --> [constant, linear, no trend, constant and linear]\n",
    "                    enforce_invertibility=False, \n",
    "                    enforce_stationarity=False).fit()\n",
    "\n",
    "print(sarimax_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ AIC = -329.965, buscamos que este valor sea lo más bajo posible\n",
    "+ Todos los coeficientes del modelo son siginificativos\n",
    "+ Este se trata del modelo SARIMAX más simple posible que se adecua al comportamiento de la serie temporal\n",
    "+ Variables impulso permiten modelar el comportamiento del efecto del covid-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residuals analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residual error\n",
    "plt.figure(figsize=[15,15])\n",
    "FT.check_residuals(pd.DataFrame(sarimax_fit.resid.loc[50:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Ljung-Box test indica que los residuos son independientes (p-valor > alpha -> No rechazamos H_0:{Residuos independientes})\n",
    "+ Como ya se comentó, existe comportamiento no modelado en la serie temporal, de nuevo, esto se puede observar tanto en el plot de los residuos como en los plot de los ACF y PACF\n",
    "+ Residuos siguen una distribucion aproximadamente normal\n",
    "+ De ACF y PACF se observa que el residuo no es ruido blanco, existen correlaciones significativas -> No se ha modelado todo el comportamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain forecasts for in-sample and out-of-sample\n",
    "# Inicio y horizonte\n",
    "start = 200\n",
    "horizon = 20\n",
    "end = df_ts.shape[0] + horizon\n",
    "\n",
    "# Exog variables\n",
    "exp2Pred = np.concatenate((exp, np.zeros((horizon,numExog))))\n",
    "\n",
    "pred = sarimax_fit.get_prediction(start=start, end= end, exog= exp[start:start+horizon+1], dynamic=False)\n",
    "yhat = pred.predicted_mean\n",
    "yhat_conf_int = pred.conf_int(alpha=0.05)\n",
    "\n",
    "#Undo Box-cox transform if necessary\n",
    "if BOX_COX:\n",
    "    yhat = sp.special.inv_boxcox(yhat, lmbda)\n",
    "    yhat_conf_int = sp.special.inv_boxcox(yhat_conf_int, lmbda)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.fill_between(yhat_conf_int.index,\n",
    "                yhat_conf_int.iloc[:, 0],\n",
    "                yhat_conf_int.iloc[:, 1], color='k', alpha=.2)\n",
    "plt.plot(df_ts.loc[start:])\n",
    "plt.plot(yhat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Se puede observar el valor real de la serie asi como el predicho. Gracias a las variables impulso el efecto inicial de la pandemia es modelado correctamente, algo que tambien ha podido observarse previamente el la grafica de los residuos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction of out_of_sample and confidence intervals\n",
    "# If using dynamic = True, the forecast are used as real data\n",
    "horizon = 20\n",
    "end = df_ts.shape[0] + horizon\n",
    "\n",
    "# COMANDO PREDICCION\n",
    "pred = sarimax_fit.get_forecast(steps=horizon, exog= exp[start:start+horizon], dynamic=False)\n",
    "yhat = pred.predicted_mean\n",
    "yhat_conf_int = pred.conf_int(alpha=0.05)\n",
    "\n",
    "#Undo Box-cox transform if necessary\n",
    "if BOX_COX:\n",
    "    yhat = sp.special.inv_boxcox(yhat, lmbda)\n",
    "    yhat_conf_int = sp.special.inv_boxcox(yhat_conf_int, lmbda)\n",
    "\n",
    "\n",
    "plt.figure(figsize=[15,15])\n",
    "plt.fill_between(yhat_conf_int.index,\n",
    "                yhat_conf_int.iloc[:, 0],\n",
    "                yhat_conf_int.iloc[:, 1], color='k', alpha=.2)\n",
    "plt.plot(df_ts.loc[1000:])\n",
    "plt.plot(yhat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# November 2022 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediccion para steps = 1 (Escala 1 e6)\n",
    "predPreBC = sarimax_fit.get_forecast(steps=1, exog = np.zeros(numExog), dynamic=False).predicted_mean\n",
    "if BOX_COX:\n",
    "    pred = sp.special.inv_boxcox(predPreBC, lmbda)\n",
    "\n",
    "print('Prediction:', round(pred.values[0]), 'parados en Noviembre de 2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparacion Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pesar de ser dos modelos similares existen diferencias entre ellos\n",
    "+ El valor del AIC es inferior en el caso del SARIMAX, esto se debe a que el efecto inicial de la pandemia (donde en SARIMA se producian errores significativos) es modelado de manera mas precisa y por tanto, disminuye esta valor.\n",
    "+ En ambos casos se tratan de los modelos mas simples que ajustan la serie temporal donde ademas todos los coeficientes son significativos.\n",
    "+ En cuanto a los residuos, observando los ACF y PACF podemos determinar que en ambos casos no se trata de ruido blanco, es decir, no se ha modelado todo el comportamineto. En el caso del SARIMAX, podemos ver una reduccion de los residuos asociados al efecto del Covid-19 (gracias a las variables impulso) con respecto a los residuos obtenidos en el SARIMA. Finalmente, en ambos casos son aproximadamente normales, con la diferencia de que los outliers observados en los residuos para el SARIMA (asociados al Covid-19) no se encuentran en el SARIMAX.\n",
    "+ Por ultimo, en cuanto a las predicciones obtenidas por ambos modelos, estas son muy similares con la diferencia de que SARIMAX ajusta mejor los valores de la fase inicial del covid-19. Las predicciones para Noviembre de 2022 varian aproximadamente en 8000 personas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Linear Model (Time series regression model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Para dicho modelo tomaremos en cuanta unicamente valores lageados de la misma serie (1, 2, 11, 12 y 13 instantes)\n",
    "+ Habria sido recomendable incorporar otras variables (asi como sus valores lageados) como por ejemplo el valor del PIB en cada mes, por falta de tiempo y recursos no ha sido posible llevarlo a cabo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load aditional data (PIB), shift 1, 2, 11, 12, 13 times Total\n",
    "dfT = df[['TOTAL']].copy()\n",
    "dfT['TOTAL_lag1'] = dfT['TOTAL'].shift()\n",
    "dfT['TOTAL_lag2'] = dfT['TOTAL'].shift(2)\n",
    "dfT['TOTAL_lag11'] = dfT['TOTAL'].shift(11)\n",
    "dfT['TOTAL_lag12'] = dfT['TOTAL'].shift(12)\n",
    "dfT['TOTAL_lag13'] = dfT['TOTAL'].shift(13)\n",
    "\n",
    "dfT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missingnvalues from shifting\n",
    "dfT.dropna(inplace=True)\n",
    "dfT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output matrices\n",
    "OUTPUT = 'TOTAL'\n",
    "INPUTS = list(dfT.drop(columns = OUTPUT).columns.values)\n",
    "\n",
    "X = dfT[INPUTS]\n",
    "y = dfT[OUTPUT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into training and test sets sequentialy\n",
    "# Create random 80/20 split\n",
    "X_train = X.iloc[0:round(0.8*X.shape[0])]\n",
    "X_test = X.iloc[round(0.8*X.shape[0])+1:X.shape[0]]\n",
    "y_train = y.iloc[0:round(0.8*X.shape[0])]\n",
    "y_test = y.iloc[round(0.8*X.shape[0])+1:X.shape[0]]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Como conjunto de entrenamiento tomaremos el primer 80% de los datos y como test el ultimo 20%. Debido a que no se escogen valores en el rango de tiempo asociado al covid-19, es de esperar que la prediccion para estos valores no sea muy precisa. Para solucionar este problema, se podria tratar de medir dicho efecto con los valores de las varibales impulso obtenidas en SARIMAX (habría que transformarlos pues estos se corresponden a la serie transformada y diferenciada) y tenerlos en cuenta para la prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset to store model predictions\n",
    "dfTR_eval = X_train.copy()\n",
    "dfTR_eval['Y'] = y_train.copy() \n",
    "dfTS_eval = X_test.copy()\n",
    "dfTS_eval['Y'] = y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Regression Model\n",
    "\n",
    "# Inputs of the model. \n",
    "INPUTS_NUM = INPUTS.copy()\n",
    "INPUTS_CAT = []\n",
    "INPUTS_MLP = INPUTS_NUM + INPUTS_CAT\n",
    "\n",
    "param = {'MLP__alpha': [0.0001,0.001,0.01], # Initial value of regularization\n",
    "         'MLP__hidden_layer_sizes':[(5,),(13,),(20,),(25,)]} # Number of neurons in each hidden layer, enters as tuples\n",
    "     \n",
    "\"\"\"\n",
    "# Uncomment the two following lines for training a single model\n",
    "param = {'MLP__alpha': [0], # Initial value of regularization\n",
    "         'MLP__hidden_layer_sizes':[(7,)]} # Number of neurons in each hidden layer, enters as tuples\n",
    "\"\"\"\n",
    "\n",
    "# Scale data previous to fit and oneHOT\n",
    "# Prepare numeric variables by scaling values\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "# Prepare the categorical variables by encoding the categories\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', drop = 'first'))])\n",
    "# Create a preprocessor to perform the steps defined above\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer, INPUTS_NUM),\n",
    "        ('cat', categorical_transformer, INPUTS_CAT)\n",
    "        ])\n",
    "pipe = Pipeline(steps=[('preprocessor',preprocessor), # Preprocess the variables when training the model \n",
    "                       ('MLP', MLPRegressor(solver='lbfgs', # Update function\n",
    "                                             activation='logistic', # Logistic sigmoid activation function\n",
    "                                             #alpha=0.0001, # L2 regularization term\n",
    "                                             #learning_rate='adaptive', # Type of learning rate used in training\n",
    "                                             max_iter=4500, # Maximum number of iterations\n",
    "                                             #batch_size=10, # Size of batch when training\n",
    "                                             tol=1e-4, # Tolerance for the optimization\n",
    "                                             #n_iter_no_change=10, # Maximum number of epochs to not meet tol improvement\n",
    "                                             # random_state=150, # For replication\n",
    "                                             verbose = True))]) #Print progress\n",
    "\n",
    "# We use Grid Search Cross Validation to find the best parameter for the model in the grid defined \n",
    "nFolds = 10\n",
    "MLP_fit = GridSearchCV(estimator=pipe, # Structure of the model to use\n",
    "                       param_grid=param, # Defined grid to search in\n",
    "                       n_jobs=-1, # Number of cores to use (parallelize)\n",
    "                       scoring='neg_mean_squared_error', # RMSE \n",
    "                       cv=nFolds) # Number of Folds \n",
    "\n",
    "# Scale objective variable\n",
    "outScaler = StandardScaler()\n",
    "y_train_scaled = outScaler.fit_transform(X = pd.DataFrame(y_train))\n",
    "\n",
    "MLP_fit.fit(X_train[INPUTS_MLP], y_train_scaled) # Search in grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Para entrenar las redes neuronales es necesario estandarizar los valores objetivos tambien, en otro caso, estas no son capaces de aprender su comportamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot grid error\n",
    "MT.plotModelGridError(MLP_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Del plot del error anterior podemos deducir la importancia del numero de perceptrones pues a mas numero de estos ultimos, se reduce el error obtenido\n",
    "+ El valor de alpha parece no ser tan significativo pues los errores obtenidos para cada red son parecidos independientemente del alpha\n",
    "+ De la combinacion de los dos graficos deducimos que necesita muchos perceptrones y un alpha bajo aunque ninguno de estos en exceso, pues esto llevaria a un sobreentrenamiento como se ve reflejado en ambas graficas produciendose un aumento del error (en el caso de la del alpha se observa ligeramente) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Analysis\n",
    "\n",
    "mlp = MLP_fit.best_estimator_['MLP']\n",
    "wts = mlp.coefs_\n",
    "bias = mlp.intercepts_\n",
    "actfunc = ['identity',MLP_fit.best_estimator_['MLP'].get_params()['activation'],mlp.out_activation_]\n",
    "X = MLP_fit.best_estimator_['preprocessor'].transform(X_train) # Preprocess the variables\n",
    "coefnames = X_train.columns.values.tolist()\n",
    "X = pd.DataFrame(X, columns=coefnames)\n",
    "y = pd.DataFrame(y_train, columns=['Y'])\n",
    "sens_end_layer = 'last'\n",
    "sens_end_input = False\n",
    "sens_origin_layer = 0\n",
    "sens_origin_input = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensmlp = ns.jacobian_mlp(wts, bias, actfunc, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensmlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensmlp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensmlp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ De las graficas obtenidas se puede identificar que variables (valores retardados) son mas significativos para la predicción del siguiente instante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a report of the model based on predictions\n",
    "# Rescale output\n",
    "\n",
    "dfTR_eval['MLP_pred'] = outScaler.inverse_transform(pd.DataFrame(MLP_fit.predict(X_train)))\n",
    "dfTS_eval['MLP_pred'] = outScaler.inverse_transform(pd.DataFrame(MLP_fit.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and test errors\n",
    "\n",
    "print('Training MAE:',mean_absolute_error(dfTR_eval['Y'], dfTR_eval['MLP_pred']))\n",
    "print('Test MAE:',mean_absolute_error(dfTS_eval['Y'], dfTS_eval['MLP_pred']))\n",
    "\n",
    "print('Training RMSE:',math.sqrt(mean_squared_error(dfTR_eval['Y'], dfTR_eval['MLP_pred'])))\n",
    "print('Test RMSE:',math.sqrt(mean_squared_error(dfTS_eval['Y'], dfTS_eval['MLP_pred'])))\n",
    "\n",
    "print('Training R2:',r2_score(dfTR_eval['Y'], dfTR_eval['MLP_pred']))\n",
    "print('Test R2:',r2_score(dfTS_eval['Y'], dfTS_eval['MLP_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Observamos diferencia entre train y test, se puede deber a sobreentrenamiento o al efecto del covid-19 que como es de esperar, no esta modelado de forma precisa\n",
    "+ Diferencia entre MAE y RMSE en test, puede deberse a ouliers, es probable que se correspondan a los valores asociados al covid-19\n",
    "+ El modelo explica de forma correcta la varianza de la variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions of the model\n",
    "\n",
    "sns.scatterplot(x='TOTAL_lag1', y='Y', data=dfTR_eval, color='black', alpha=0.5)\n",
    "sns.scatterplot(x='TOTAL_lag1', y='MLP_pred', data=dfTR_eval, color='red', edgecolor='black').set_title('Predictions for training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Ademas de la precision de las predicciones se puede observar la correlacion entre la variable objetivo y su valor un instante retardado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of residuals\n",
    "\n",
    "RT.plotModelDiagnosis(dfTS_eval, 'MLP_pred', 'Y', figsize = (15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ En cuanto a los residuos, observamos que oscilan en torno al 0 pero este valor no se mantiene constante en practicamente ningun grafico, identificamos tambien la presencia de outliers, esto es un indicador de que exite gran parte de comportamiento no modelado\n",
    "+ Lo ideal seria esperar que los errores siguiesen una normal de media 0 y varianza constante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction against real value\n",
    "sns.scatterplot(data=dfTS_eval, x='Y', y='Y', color='black', alpha=0.5)\n",
    "sns.scatterplot(data=dfTS_eval, x='MLP_pred', y='Y', color='blue', edgecolor='black')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Valores reales frente a los predichos, de nuevo se observan ouliers, probablemente debidos al efecto del covid-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare forecasts in time\n",
    "\n",
    "plt.figure()\n",
    "plt.plot('Y', data=dfTS_eval, label='Real')\n",
    "plt.plot('MLP_pred', data=dfTS_eval, label='Forecast')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Se observan los valores reales de la serie así como su prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlation of residuals\n",
    "\n",
    "plt.figure(figsize = [15, 15])\n",
    "FT.ts_display(dfTS_eval['Y'] - dfTS_eval['MLP_pred'], lags = 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Errores covid-19, no en entrenamiento, no esperado\n",
    "+ Residuos no ruido blanco, comportamiento no modelado\n",
    "+ Como ya hemos comentado, se podrian emplear variables como PIB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Prediction November 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastValue = df.index[-1]\n",
    "dataPred = X_test.loc[lastValue:, :]\n",
    "valuePred = outScaler.inverse_transform(pd.DataFrame(MLP_fit.predict(dataPred)))\n",
    "print('Prediction:', round(valuePred[0][0]), 'parados en Noviembre de 2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ La prediccion obtenida desentona mas con las anteriores, ofrece un valor mas alto, aproximadamente 70000 personas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediccion final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Basada en SARIMA, SARIMAX o combinacion de estas (e.g. media aritmetica)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2385fc4aef5e04ad1f7070587c7f6c5e2e88397b4696a09353ed0ed3c263b3dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
